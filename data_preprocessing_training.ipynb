{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Erevna_Usoon_version_4_data_preprocessing&training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMV5Tx26EEr5U8ReNhm3+YJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ouW2jKmOyq6m"},"source":["from google.colab import drive\r\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssVGbwAXmiZ7"},"source":["in_usoon_priv_database = \"/content/drive/My Drive/erevna/Usoon_private_database/\"\r\n","out_usoon_priv_database = \"/content/drive/My Drive/erevna/\"'\r\n","\r\n","ids=[\"malika\",\"renuka\",\"other\"]\r\n","modes=[\"train\",\"val\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"czTuHow6yt5_"},"source":["!pip install mtcnn\r\n","\r\n","!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\r\n","!bunzip2 \"shape_predictor_68_face_landmarks.dat.bz2\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGfkBseX605C"},"source":["1) Code that will allow company to extract out and save (in private database) only face of user A from rest all extracted faces from the pictures uploaded in his/her profile.<br>\r\n","\r\n","2) To do so, for first 25 times of user A uploading his/her pic in the Usoon profile, company will be asking him/her out of all extracted photos, which one is his/her own photo..........After 25 times, algorithm by itself will extract all faces from the photos uploaded by user A and will only keep user A's face. \r\n","\r\n","3) These face cutouts of user A will be beneficial to match his/her presence in some photo uploaded by user B, which in turn if validates True will save the user A in friend list of user B."]},{"cell_type":"code","metadata":{"id":"ZVadxHY_9kI9"},"source":["from matplotlib import pyplot\r\n","from mtcnn.mtcnn import MTCNN\r\n","import os\r\n","import sys\r\n","import cv2\r\n","import shutil\r\n","import numpy as np\r\n","from numpy import savez_compressed, load, asarray, expand_dims \r\n","from keras.models import load_model\r\n","from sklearn.metrics import accuracy_score\r\n","from sklearn.preprocessing import LabelEncoder,Normalizer\r\n","from sklearn.svm import SVC\r\n","from matplotlib import pyplot\r\n","import pickle\r\n","from matplotlib.patches import Rectangle\r\n","import random \r\n","from PIL import Image\r\n","import dlib"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rFUqxTYriKoG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQBkYOb3p1YB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9j9duSEoijaO"},"source":["# **Refining Collected Dataset**"]},{"cell_type":"markdown","metadata":{"id":"yeu0ibXViz4b"},"source":["**Extracting** faces from images present in collected dataset, **aligning** them to a similar pattern, **asking** user which face-cutout belongs to him/her and **saving** only those face-cutouts which belongs to user (according to users confirmation)"]},{"cell_type":"code","metadata":{"id":"AyuDjBakAK5p"},"source":["#this function will align all photos into a similar pattern\r\n","\r\n","\r\n","def face_alignment(image,faces,i):\r\n","    dimension=list()\r\n","    #saving dimensions of \"leftEyeCenter\", \"rightEyeCenter\" and \"nose\" presennt inside sub-dictionary which is value of a key \"keypoints\" of dictionary named \"faces\",\r\n","    #in list named as \"dimension\" \r\n","    for key in faces[i]['keypoints']:\r\n","      a=faces[i]['keypoints'][key]\r\n","      dimension.append(a)\r\n","    \r\n","\r\n","    #retreiving the dimemsions from list called \"dimension\", into thier respective named variables \"leftEyeCenter\", \"rightEyeCenter\" and \"nose\" \r\n","    left_eye_center=dimension[0]\r\n","    right_eye_center=dimension[1]\r\n","    nose=dimension[2]\r\n","\r\n","\r\n","    #extracting \"x-coordinate\" and \"y-coordinate\" of \"leftEyeCenter\" into \"leftEyeCenter_x\" and \"leftEyeCenter_y\" respectively   \r\n","    a=str(left_eye_center)\r\n","    aa=a.split(\"(\")[1]\r\n","    left_eye_center_x=aa.split(\",\")[0] \r\n","    left_eye_center_x=int(left_eye_center_x)\r\n","    aaa=aa.split(\",\")[1]\r\n","    aaaa=aaa.split(\" \")[1]\r\n","    left_eye_center_y=aaaa.split(\")\")[0] \r\n","    left_eye_center_y=int(left_eye_center_y)\r\n","\r\n","\r\n","    #extracting \"x-coordinate\" and \"y-coordinate\" of \"rightEyeCenter\" into \"rightEyeCenter_x\" and \"rightEyeCenter_y\" respectively\r\n","    b=str(right_eye_center)\r\n","    bb=b.split(\"(\")[1]\r\n","    right_eye_center_x=bb.split(\",\")[0]\r\n","    right_eye_center_x=int(right_eye_center_x)\r\n","    bbb=bb.split(\",\")[1]\r\n","    bbbb=bbb.split(\" \")[1]\r\n","    right_eye_center_y=bbbb.split(\")\")[0] \r\n","    right_eye_center_y=int(right_eye_center_y)\r\n","\r\n","\r\n","    # find angle of line joining centres of left and right eyes, by using slop of the line formulae.\r\n","    dy = right_eye_center_y - left_eye_center_y\r\n","    dx = right_eye_center_x - left_eye_center_x\r\n","    angle = np.degrees(np.arctan2(dY, dX))\r\n","\r\n","\r\n","    # to get the face at the center of the image, set left eye to 35% from x-axis and 35% from y-axis. Location of right eye will be found out by using left eye location.\r\n","    desired_left_eye=(0.35, 0.35)\r\n","    desired_right_eye_X = 1.0 - desired_left_eye[0]\r\n","\r\n","\r\n","    # calculating the scale of the aligned image by taking the ratio of the distance between eyes in the \"non-aligned input\" face-cutout image and \"aligned output\" \r\n","    # face-cutout image\r\n","    current_distance = np.sqrt((dx ** 2) + (dy ** 2))\r\n","    desired_distance = (desired_right_eye_X - desired_left_eye[0])\r\n","    desired_distance *= 256\r\n","    scale = desired_distance / current_distance\r\n","\r\n","\r\n","    # calculate (x, y)-coordinates of center between the two eyes in the \"non-aligned input\" face-cutout image\r\n","    center_of_eyes = ((left_eye_center_x + right_eye_center_x) // 2, (left_eye_center_y + right_eye_center_y) // 2)\r\n","\r\n","\r\n","    # calculate the rotation matrix for rotating and scaling the \"non-aligned input\" face-cutout image into \"aligned output\" face-cutout image\r\n","    M = cv2.getRotationMatrix2D(eyesCenter, angle, scale)\r\n","\r\n","\r\n","    # update the translation component of the matrix\r\n","    tX = desiredFaceWidth * 0.5\r\n","    tY = desiredFaceHeight * desiredLeftEye[1]\r\n","    M[0, 2] += (tX - eyesCenter[0])\r\n","    M[1, 2] += (tY - eyesCenter[1])\r\n","\r\n","\r\n","    # apply the affine transformation\r\n","    (w, h) = (256, 256)\r\n","    output = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC)\r\n","    \r\n","\r\n","    return(output)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JutPNz7-gEqr"},"source":["# this class will help to raise error if user tries to input anything apart from 'yes' or 'no'\r\n","\r\n","\r\n","class not_ok(Exception):\r\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMkY6qdopj8u"},"source":["# this function will help to draw-out extracted faces, ask user to recognize his face cutout out of rest all and then will either save or not save a particular \r\n","# face cutout according to user's reply. \r\n","\r\n","\r\n","def draw_faces(imagename, result):\r\n","  # we will keep noting down names of images iterated once into a notepad file so that later on, those images will not come again\r\n","  with open(in_usoon_priv_database+str(id)+\"_counter.txt\", 'a') as encoded_counter:\r\n","    encoded_counter.write(item+\"\\n\") #we will keep noting down names of images iterated once into a notepad file so that later                   \r\n","  # load the image via its image path\r\n","  data = cv2.imread(imagename)\r\n","  # initialize an empty dictionary\r\n","  d={}\r\n","  # now one-by-one we will iterate through all face cutouts detected by mtcnn in an image.\r\n","  # for first run, \"i\" means first face cutout out of rest face cutouts\r\n","\r\n","  # if you want to see non-aligned face-cutouts then uncomment all lines starting from below............. \r\n","  #for i in range(len(result)):\r\n","      #x1, y1, width, height = result[i]['box']\r\n","      #x2, y2 = x1 + width, y1 + height\r\n","      #face_cutout = data[y1:y2, x1:x2]\r\n","      #face_cutout = cv2.resize(face_cutout,(256,256))\r\n","      #print(\" \")\r\n","      #print(\" \")\r\n","      #next we will refer which face cutout is it. For first iteration it will be 1st photo/face cutout\r\n","      #conf = result[i]['confidence']\r\n","      #if conf>0.92:\r\n","        #pyplot.axis('off')\r\n","        #pyplot.imshow(face_cutout)\r\n","        #print(\"Photo numbered\",i+1)\r\n","        #print(\"face-cutout before alignment\")\r\n","        #pyplot.show()\r\n","      #else:\r\n","        #continue\r\n","  #.............upto here\r\n","\r\n","\r\n","  # this code block will help to show-and-save aligned face-cutouts\r\n","  for i in range(len(result)):\r\n","    # we will pass values to function \"face_alignment\" and collect the \"aligned face-cutout\" in container \"face_cutout_aligned\"\r\n","    face_cutout_aligned = face_alignment(data,result,i)\r\n","    print(\" \")\r\n","    print(\" \")\r\n","    # we will gave a \"unique name\" to this aligned face and save this \"unique name\" of every face cutout as a value with every key \"i\",i.e. {i: unique_name}\r\n","    # this dictionary will be later used to get reference of each face cutout in a particular order while taking user's confirmation and deciding whether to delete \r\n","    # that particular face-cutout or to let it be as it is \r\n","    a=imagename.split('/')[7]\r\n","    aa=a.split('.')[0]\r\n","    b=company_data+aa+str(\"=\")+str(i)+\".jpg\" \r\n","    d[i]=b  \r\n","    # then we will try to save this face-cutout with its \"unique name\".\r\n","    try:\r\n","      cv2.imwrite(d[i], face_cutout_aligned)\r\n","    # if failed to save the current face-cutout, we will pick next face-cutout and continue the same process with it\r\n","    except Exception:\r\n","      continue\r\n","    else:\r\n","      conf = result[i]['confidence']\r\n","      # if current face-cutout gets saved, then next step will be to see \"confidence\" value corresponding to current face-cutout\r\n","      if conf>0.92:\r\n","        # If the \"confidence\" is more than \"0.92\" then it shows that current face-cutout is of a human. And if that is the case then we will plot that face-cutout\r\n","        pyplot.axis('off')\r\n","        pyplot.imshow(face_cutout_aligned)\r\n","        print(\"Photo numbered\",i+1)\r\n","        print(\"face-cutout after alignment\")\r\n","        pyplot.show()\r\n","      else:\r\n","        continue\r\n","\r\n","\r\n","  for i in range(len(result)):\r\n","    # we will only pop-up confirmation question for face-cutouts which without showing error in \"try...else\" statement (in above code lines), got saved in the database\r\n","    if os.path.exists(d[i]):\r\n","        while True:\r\n","          try:    \r\n","              print(\" \")\r\n","              print(\" \")    \r\n","              # now we will ask user which face cutout is of him/her           \r\n","              confirm = input(\"Photo numbered \"+str(i+1)+\" is you? ('1' for 'yes'/ '2' for 'no'/ '3' for 'will resume later on' ):- \")\r\n","              for elem in confirm.split():\r\n","                  # if user tries to input anything other than '1' (for 'yes') or '2' (for 'no') or '3' (for 'resuming later'), in reply to our confirmation question,\r\n","                  # an error will be raised (whose class we have already defined above) saying that you can only reply either 'yes' or 'no'\r\n","                  if not (elem==\"1\" or elem==\"2\" or elem==\"3\"):\r\n","                    raise not_ok\r\n","                  # if user replied to our confirmation question in '1' or '2' or '3', then we will break from here and will go\r\n","                  # to 'else' present below 'except'...\r\n","                  break\r\n","          except not_ok:\r\n","              print(\"Sorry, type in '1' for 'yes'/ '2' for 'no'/ '3' for 'will resume later on'.\")\r\n","          else:\r\n","              #...in this line\r\n","              # if user replied '1', we will leave the saved cutout untouched....note:- we have saved face-cutouts earlier via code-lines mentioned above and \r\n","              # now we are deciding whether to move them away from their current destination to non-caring folder named \"other\", or to leave them there only untouched\r\n","              if elem==\"1\":\r\n","                print(\"saved image\")\r\n","                print(\"\")\r\n","                print(\"\")\r\n","              #if user replied '2', then corresponding face-cutout will move to \"other\" folder   \r\n","              elif elem==\"2\":\r\n","                shutil.move(d[i], in_usoon_priv_database+\"other\")\r\n","                print(\"not saved image\") \r\n","                print(\"\")\r\n","                print(\"\") \r\n","              else:\r\n","                # if user replied '3', then we will delete the latest entry of the face cutout from the notepad \".._counter.txt\", so that when user resume the confirmation prcocess he/she\r\n","                # at some point of time wuld be able to encounter image on which he left the process in mid-way, thus using available data very carefully without wasting it.\r\n","                # After deleting latest entry from notepad \".._counter.txt\", we will stop our program\r\n","                count=0\r\n","                with open(in_usoon_priv_database+str(id)+\"_counter.txt\", 'r') as encoded_counter:\r\n","                  line_list = encoded_counter.readlines()\r\n","                  a=len(line_list)\r\n","                  with open(\"/content/drive/My Drive/erevna/Usoon_private_database/\"+str(id)+\"_counter.txt\", 'w') as encoded_counterr:\r\n","                    for line in line_list:\r\n","                      count=count+1\r\n","                      if count!=a:\r\n","                        encoded_counterr.writelines(line)\r\n","                sys.exit()\r\n","              break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwqxWKJFvu86"},"source":["# driver code\r\n","\r\n","\r\n","# here we are asking user to input his/her user ID \r\n","id = input(\"please enter your Usoon ID:- \")\r\n","\r\n","# location for company_data containing face cutouts of user\r\n","company_data = in_usoon_priv_database+str(id)+\"/\"\r\n","# if there will not be any folder in company_data to store face cutouts of user, then folder will be formed  \r\n","if not os.path.exists(company_data):\r\n","    os.makedirs(company_data)\r\n","\r\n","# location for company_data containing random pics\r\n","company_data_other = in_usoon_priv_database+\"other/\"\r\n","if not os.path.exists(company_data_other):\r\n","    os.makedirs(company_data_other)\r\n","\r\n","# location for user_data containing images of user (alone or with someone else) got either from his/her Usoon profile or from face recorder\r\n","user_data = os.listdir(out_usoon_priv_database + str(id)+'/pics')\r\n","\r\n","# we will store names of all images iterated till now in list called 'names'\r\n","try:\r\n","  names=[]\r\n","  with open(in_usoon_priv_database+str(id)+\"_counter.txt\", 'r') as f:\r\n","    lines = f.readlines()\r\n","    for line in lines:\r\n","      linee=line.split('/')[7]\r\n","      lineee=linee.split('\\n')[0]\r\n","      names.append(lineee)\r\n","except Exception:\r\n","  names=[]\r\n","\r\n","\r\n","a=1\r\n","# we take first image of user from 'user_data' \r\n","for entry in user_data:\r\n","  # just pick-out name (without '.jpg')\r\n","  entryyy = entry.split('.')[0]\r\n","  # make full path of image (came from using 'for loop' on 'user_data')\r\n","  item =out_usoon_priv_database+str(id)+'/pics/'+ str(entry)\r\n","  # load that image from 'user_data'\r\n","  image_loadedd = cv2.imread(item)\r\n","  # create the MTCNN detector, using default weights and detect faces in images\r\n","  face_detector = MTCNN()\r\n","  faces = face_detector.detect_faces(image_loadedd)\r\n","  if a<2 and len(faces)<5 and len(faces)>0 and entry not in names:\r\n","    if len(faces)<2:\r\n","      continue\r\n","    else:\r\n","      a=4\r\n","      draw_faces(item, faces)\r\n","  elif a>=2 and len(faces)<5 and len(faces)>0 and entry not in names:\r\n","    draw_faces(item, faces)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P7AjqPr2dU6U"},"source":["To see number of face-cutouts in 'parent' folder of each user"]},{"cell_type":"code","metadata":{"id":"h_4HldaX7MnX"},"source":["for id in ids:\r\n","  c = os.listdir(in_usoon_priv_database+str(id))\r\n","  print(\"number of face-cutouts in \"+str(id)+\"'s parent folder are:\"+str(len(c)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-XVM0WrrgHi7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFND79Auhosp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHN9n4LF7Nek"},"source":["To **split** processed face-cutouts of user from company's database into **validation dataset** and **test dataset**.  \r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"zsJ2ahnado-G"},"source":["# function that will split face cutouts of an user into validation dataset and test dataset\r\n","def split(id):\r\n","    # load the face-cutouts of an user from 'company_data' \r\n","    company_data = os.listdir(in_usoon_priv_database+str(id))\r\n","\r\n","\r\n","    # create path of 'company_data_train' folder where some face-cutouts will be stored for 'training' purpose   \r\n","    company_data_train = in_usoon_priv_database+str(id)+\"_train\"\r\n","    if not os.path.exists(company_data_train):\r\n","        os.makedirs(company_data_train)\r\n","\r\n","\r\n","    # create path of 'company_data_val' folder where some face-cutouts will be stored for 'testing' purpose   \r\n","    company_data_val = in_usoon_priv_database+str(id)+\"_val\"\r\n","    if not os.path.exists(company_data_val):\r\n","        os.makedirs(company_data_val)\r\n","\r\n","\r\n","    # these containers will store names of all images stored in 'company_data_val' and 'company_data_train'\r\n","    company_data_vall = os.listdir(in_usoon_priv_database+str(id)+\"_val/\")\r\n","    company_data_trainn = os.listdir(in_usoon_priv_database+str(id)+\"_train/\")\r\n","\r\n","\r\n","    # this variable will be used later\r\n","    count=0\r\n","\r\n","\r\n","    # we will pick finalised face-cutouts and save first 30s for validation process and rest all for training process\r\n","    for x in company_data:\r\n","      xx=in_usoon_priv_database+id+\"/\"+str(x)\r\n","      count=count+1\r\n","      if count<31:\r\n","        shutil.move(xx, in_usoon_priv_database+str(id)+\"_val\")    \r\n","      else:\r\n","        shutil.move(xx, in_usoon_priv_database+str(id)+\"_train\")\r\n","\r\n","\r\n","\r\n","\r\n","# driver code\r\n","for id in ids:\r\n","    split(id)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9CLYjymaeuW5"},"source":["To see number of face-cutouts in 'train' and 'val' folder of each user."]},{"cell_type":"code","metadata":{"id":"V5sdlu85ettq"},"source":["for id in ids:\r\n","  for mode in modes:\r\n","    c = os.listdir(in_usoon_priv_database+str(id)+\"_\"+str(mode))\r\n","    print(\"number of face-cutouts in \"+str(id)+\"'s \"+str(mode)+\" folder are:\"+str(len(c)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YCc2t8sx7FIz"},"source":["To **undo** above action and proof:- all 'train' and 'val' folders will be empty"]},{"cell_type":"code","metadata":{"id":"w90iqQMc2_eK"},"source":["for id in ids:\r\n","  for mode in modes:\r\n","    a = os.listdir(in_usoon_priv_database+str(id)+\"_\"+str(mode))\r\n","    b = in_usoon_priv_database+str(id)\r\n","\r\n","    for x in a:\r\n","      xx=in_usoon_priv_database+str(id)+\"_\"+str(mode)+\"/\"+str(x) \r\n","      shutil.move(xx, b)\r\n","\r\n","\r\n","for id in ids:\r\n","  for mode in modes:\r\n","    c = os.listdir(in_usoon_priv_database+str(id)+\"_\"+str(mode))\r\n","    print(\"number of face-cutouts in \"+str(id)+\"'s \"+str(mode)+\" folder are:\"+str(len(c)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjHyAm6fhmmH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jx5j7CYKhmWl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G8nITPNQ-qTq"},"source":["To convert processed train and validation face-cutouts datasets into corresponding **numpy** and then compressing it all together."]},{"cell_type":"code","metadata":{"id":"s1FfoKYDorui"},"source":["# this function firt \"resize\" and then \"arrays\" every single face-cutouts present in \"renuka_train\",\"renuka_val\",\"malika_train\" and \"malika_val\" folders \r\n","# then save each folders'(mentioned above)resized&arrayed elements into two lists and return these two lists, when called four times \r\n","\r\n","\r\n","def arrays(mode,id):\r\n","  face_numpy = list()\r\n","  faces = list()\r\n","  label = list()\r\n","  company_data = os.listdir(in_usoon_priv_database+str(id)+\"_\"+str(mode))\r\n","  for x in company_data:\r\n","    xx=in_usoon_priv_database+str(id)+\"_\"+str(mode)+\"/\"+str(x)\r\n","    image_loaded = cv2.imread(xx)\r\n","    try:\r\n","      image_loadedd = cv2.resize(image_loaded,(160, 160))\r\n","    except Exception:\r\n","      continue\r\n","    face_cutout = asarray(image_loadedd)\r\n","    faces.append(face_cutout)\r\n","    label.append(id)\r\n","  return faces,label\r\n","\r\n","\r\n","\r\n","\r\n","d={}\r\n","# we are converting all face-cutouts into array form. \r\n","# after that we keep all \"arrayed training face-cutouts\", all \"arrayed validation face-cutouts\",\r\n","# all \"labels corresponding to training face-cutouts\" and all \"labels corresponding to validation face-cutouts\"\r\n","for mode in modes:\r\n","  # we take a particular \"mode\" (like \"train\") and initialise two lists \"numpy_face\" and \"numpy_label\".\r\n","  # thus for \"train\" there will be different \"numpy_face\" and \"numpy_label\" lists....and seperate for \"val\" mode.\r\n","  numpy_face = list()\r\n","  numpy_label = list()\r\n","  # now we call all \"training face-cutouts\" realted to first id   \r\n","  for id in ids:\r\n","    # call the function \"arrays\", which will return \"all arrayed training face-cutouts\" list \"faces\" and \"all labels \r\n","    # corresponding to training face-cutouts\" list \"label\" in forms \"numpy_faces\" and \"numpy_labels\" respectively \r\n","    numpyy_faces,numpyy_labels = arrays(mode,id)\r\n","    # next we add list \"numpyy_faces\" (containing \"all arrayed training face-cutouts\") into \"numpy_face\" list and \r\n","    #\"numpyy_labels\" (containing \"all labels corresponding to training face-cutouts\") into \"numpy_label\". \r\n","    # Later in these senior-most lists, \"all arrayed validation face-cutouts\" and \"all labels corresponding to validation\r\n","    # face-cutouts\" will be added.\r\n","    numpy_face.extend(numpyy_faces)\r\n","    numpy_label.extend(numpyy_labels)\r\n","\r\n","  # just arraying senior-most lists \"numpy_face\" and 'numpy_label\" \r\n","  numpy_faces = asarray(numpy_face)\r\n","  numpy_labels = asarray(numpy_label)\r\n","  numpy_face_key=\"numpy_face_\"+str(mode)\r\n","  numpy_label_key=\"numpy_label_\"+str(mode)\r\n","  # and then these \"arrayed senior-most lists\" will be saved as \"value\" corresponding to dictionary keys of \r\n","  # \"numpy_face_train\", \"numpy_face_val\", \"numpy_label_train\" and \"numpy_label_val\"\r\n","  d[numpy_face_key]=numpy_faces\r\n","  d[numpy_label_key]=numpy_labels\r\n","\r\n","\r\n","print(\"Shape of numpy_face_train is:- \"+str(d['numpy_face_train'].shape));print(\"Shape of numpy_label_train is:- \"+str(d['numpy_label_train'].shape));print(\"Shape of numpy_face_val is:- \"+str(d['numpy_face_val'].shape));print(\"Shape of numpy_label_val is:- \"+str(d['numpy_label_val'].shape))  \r\n","filename_for_saving = in_usoon_priv_database+\"numpy_compressed_dataset.npz\"\r\n","# then we save these four lists by calling them through corresponding dictionary key.\r\n","savez_compressed(filename_for_saving, d['numpy_face_train'], d['numpy_label_train'], d['numpy_face_val'], d['numpy_label_val'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cai_9X4AMfh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9GTlsk4omZN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"20NTJ9uSNAvK"},"source":["Load back the numpy compressed dataset, convert to **facenet-embeddings**, again compress and save\r\n"]},{"cell_type":"code","metadata":{"id":"colNJV9GA3ya"},"source":["# takes \"arrayed training and validation face-cutouts\" and return back corresponding embeddings to \"collect_embedding\"\r\n","\r\n","\r\n","def numpy_to_embedding(model, numpy_array):\r\n","  numpy_array = numpy_array.astype('float32')\r\n","  mean, std = numpy_array.mean(), numpy_array.std()\r\n","  numpy_array = (numpy_array - mean) / std\r\n","  numpy_array = expand_dims(numpy_array, axis=0)\r\n","  yhat = model.predict(numpy_array)\r\n","  face_embedding=yhat[0]\r\n","  return face_embedding\r\n","\r\n","\r\n","# collect embeddings of firstly \"arrayed training face-cutouts\" from \"numpy_to_embedding\", append in list \"container\", return that list to \r\n","# \"driver code\"......then do same for \"arrayed validation face-cutouts\"\r\n","def collect_embedding(model,numpy_array):\r\n","  container=list()\r\n","  a=bag[numpy_array]\r\n","  for b in a:\r\n","    embedding = numpy_to_embedding(model,b)\r\n","    container.append(embedding)\r\n","    containerr = asarray(container)\r\n","  return containerr\r\n","\r\n","\r\n","\r\n","\r\n","model = load_model(out_usoon_priv_database+'facenet_keras.h5'\r\n","# we load back the \"arrayed dataset\" into 4 containers and save them as \"values\" to cooresponding \"keys\" in \"bag\" dictionary\r\n","data = load(in_usoon_priv_database+\"numpy_compressed_dataset.npz\")\r\n","numpy_face_train, numpy_label_train, numpy_face_val, numpy_label_val = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\r\n","# print(\"numpy_face_train_shape:- \",numpy_face_train.shape);print(\"numpy_label_train_shape:- \",numpy_label_train.shape);print(\"numpy_face_val_shape:- \",numpy_face_val.shape);print(\"numpy_label_val_shape:-  \",numpy_label_val.shape) \r\n","bag={}\r\n","bag['numpy_face_train']=numpy_face_train\r\n","bag['numpy_face_val']=numpy_face_val\r\n","bag['numpy_label_train']=numpy_label_train\r\n","bag['numpy_label_val']=numpy_label_val\r\n","\r\n","bucket={}\r\n","listtt=['face_train', 'face_val']\r\n","# now we call \"collect_embedding\" function, who will return \"embeddings of all training face-cutouts\"\r\n","# and \"embeddings of all validation face-cutouts\". We will save these returned embeddings into \"bucket\" dictionary as \"value\"\r\n","# to corresponding \"keys\"\r\n","for name in listtt:\r\n","  namee=\"numpy_\"+name\r\n","  bucket_key = \"embed_\"+name\r\n","  bucket[bucket_key]=collect_embedding(model, namee)\r\n","\r\n","\r\n","# next we pick-up training&validation face-cutouts embeddings from dictionary \"bucket\" and save them into \"embed_face_train\" \r\n","# and \"embed_face_val\".....and pick-up training&validation face-cutouts labels from dictionary \"bag\" and save them into \r\n","# \"embed_label_train\" and \"embed_label_val\"\r\n","embed_face_train=bucket['embed_face_train']\r\n","embed_face_val=bucket['embed_face_val']\r\n","embed_label_train=bag['numpy_label_train']\r\n","embed_label_val=bag['numpy_label_val']\r\n","print(\" \")\r\n","print(\" \")\r\n","print('embed_face_train_shape:- ',embed_face_train.shape);print('embed_label_train_shape:- ',embed_label_train.shape);print('embed_face_val_shape:- ',embed_face_val.shape);print('embed_label_val_shape:- ',embed_label_val.shape)\r\n","\r\n","\r\n","# then save \"training&validation face cutouts embeddings\" and \"training&validation face cutouts labels\"\r\n","filename_for_saving = in_usoon_priv_database+\"embedding_compressed_dataset.npz\"\r\n","savez_compressed(filename_for_saving, embed_face_train, embed_label_train, embed_face_val, embed_label_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxsEArUR7ZoZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FagKXpy9AClN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fcs36ZqIA-_B"},"source":["# **Training & Validating**"]},{"cell_type":"markdown","metadata":{"id":"VEM04eLLHhVe"},"source":["**Implementating SVM** on embedded test and train datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceYnxaugNTmi","executionInfo":{"status":"ok","timestamp":1611596287795,"user_tz":-330,"elapsed":1790,"user":{"displayName":"Malay Joshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwuEZFx4BpLgLygZI6c-ezHM3OzHzCsshgGbFtuw=s64","userId":"07941266660135468355"}},"outputId":"b2289d13-f30f-4a2a-c7e7-84a456340a6d"},"source":["data = load(in_usoon_priv_database+'embedding_compressed_dataset.npz')\r\n","embed_face_train, embed_label_train, embed_face_val, embed_label_val = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\r\n","# print('embed_face_train_shape',embed_face_train.shape);print('embed_label_train_shape',embed_label_train.shape);print('embed_face_val_shape',embed_face_val.shape);print('embed_label_val_shape',embed_label_val.shape)\r\n","# normalize arrayed face-cutouts\r\n","in_encoder = Normalizer(norm='l2')\r\n","embed_face_trainn = in_encoder.transform(embed_face_train)\r\n","embed_face_vall = in_encoder.transform(embed_face_val)\r\n","# label encoding and saving this \"tool\" for future use\r\n","out_encoder = LabelEncoder()\r\n","out_encoder.fit(embed_label_train)\r\n","embed_label_trainn = out_encoder.transform(embed_label_train)\r\n","embed_label_vall = out_encoder.transform(embed_label_val)\r\n","numpy.save(in_usoon_priv_database+'classes.npy', out_encoder.classes_)\r\n","\r\n","\r\n","# training the model and saving it for future use\r\n","model = SVC(kernel='linear', probability=True)\r\n","model.fit(embed_face_trainn, embed_label_trainn)\r\n","with open(in_usoon_priv_database+\"model.pkl\", \"wb\") as encoded_pickle:\r\n","    pickle.dump(model, encoded_pickle)\r\n","\r\n","\r\n","# using the trained model to predict labels\r\n","yhat_train = model.predict(embed_face_trainn)\r\n","yhat_val = model.predict(embed_face_vall)\r\n","# calculating accuracy score and printing it\r\n","score_train = accuracy_score(embed_label_trainn, yhat_train)\r\n","score_val = accuracy_score(embed_label_vall, yhat_val)\r\n","print(\" \")\r\n","print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_val*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" \n","Accuracy: train=98.431, test=97.778\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ftZP6pyb7qqu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfTflqIBAA42"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ALUhCHb5AWLt"},"source":["# **Testing**"]},{"cell_type":"code","metadata":{"id":"WMQKEPRLULzX"},"source":["idd=\"renuka\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RahBD5vXGSeV"},"source":["def draw_and_numpy_faces(data, result, imagename):\r\n","  d={}\r\n","  face_numpy=list()\r\n","  face_dimension=list()\r\n","  face_id=list()\r\n","  face_score=list()\r\n","\r\n","  for i in range(len(result)):\r\n","      x1, y1, width, height = result[i]['box']\r\n","      score=result[i]['confidence']\r\n","      x2, y2 = x1 + width, y1 + height\r\n","      imagee = data[y1:y2, x1:x2]\r\n","      face_cutout = cv2.resize(imagee,(160, 160))   \r\n","      face_cutout_array = asarray(face_cutout)\r\n","      face_numpy.append(face_cutout_array)\r\n","      face_dimension.append(result[i]['box'])\r\n","      face_id.append(imagename)\r\n","      face_score.append(score)\r\n","  return face_numpy,face_dimension,face_id,face_score\r\n","\r\n","\r\n","def numpyy(test_data):\r\n","    face_numpy_collect=list()\r\n","    face_id_collect=list()\r\n","    face_dimension_collect=list()\r\n","    face_score_collect=list()\r\n","\r\n","    for entry in test_data:\r\n","      entryyy = entry.split('.')[0]\r\n","      item = out_usoon_priv_database+'test_pics/'+ str(entry)\r\n","      image_loadedd = cv2.imread(item)\r\n","      face_detector = MTCNN()\r\n","      faces = face_detector.detect_faces(image_loadedd)\r\n","      if len(faces)<5 and len(faces)>0 :\r\n","        face_numpy,face_dimension,face_id,face_score = draw_and_numpy_faces(image_loadedd, faces, item)\r\n","        face_numpy_collect.extend(face_numpy)\r\n","        face_dimension_collect.extend(face_dimension)\r\n","        face_id_collect.extend(face_id)\r\n","        face_score_collect.extend(face_score)\r\n","    face_numpy_collect = asarray(face_numpy_collect)\r\n","    face_dimension_collect = asarray(face_dimension_collect)\r\n","    face_id_collect = asarray(face_id_collect)\r\n","    face_score_collect = asarray(face_score_collect)\r\n","    print(\"numpy_face_train_shape:- \",face_numpy_collect.shape)\r\n","    return face_numpy_collect,face_dimension_collect,face_id_collect,face_score_collect\r\n","\r\n","\r\n","\r\n","\r\n","#location for test_data containing whole image of user (alone or with someone else)\r\n","test_data = os.listdir(out_usoon_priv_database+'test_pics/')\r\n","numpyy_pic,face_dimension_collect,face_id_collect,face_score_collect=numpyy(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUfG9lZUf_q1","executionInfo":{"status":"ok","timestamp":1611603087915,"user_tz":-330,"elapsed":8810,"user":{"displayName":"Malay Joshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwuEZFx4BpLgLygZI6c-ezHM3OzHzCsshgGbFtuw=s64","userId":"07941266660135468355"}},"outputId":"dc2e59af-a3d1-450f-d5ff-6762aacefd98"},"source":["def numpy_to_embedding(model, numpy_array):\r\n","  numpy_array = numpy_array.astype('float32')\r\n","  mean, std = numpy_array.mean(), numpy_array.std()\r\n","  numpy_array = (numpy_array - mean) / std\r\n","  numpy_array = expand_dims(numpy_array, axis=0)\r\n","  yhat = model.predict(numpy_array)\r\n","  face_embedding=yhat[0]\r\n","  return face_embedding\r\n","\r\n","\r\n","def collect_embedding(model,numpy_array):\r\n","  container=list()\r\n","  for b in numpy_array:\r\n","    embedding = numpy_to_embedding(model,b)\r\n","    container.append(embedding)\r\n","    containerr = asarray(container)\r\n","  return containerr\r\n","\r\n","\r\n","\r\n","\r\n","model = load_model(out_usoon_priv_database+'facenet_keras.h5')\r\n","embed_face_test=collect_embedding(model, numpyy_pic)\r\n","print('embed_face_test_shape:- ',embed_face_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","embed_face_test_shape:-  (15, 128)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ae8JLSXuAZ0F"},"source":["in_encoder = Normalizer(norm='l2')\r\n","embed_face_test = in_encoder.transform(embed_face_test)\r\n","\r\n","\r\n","out_encoder = LabelEncoder()\r\n","out_encoder.classes_ = numpy.load(in_usoon_priv_database+'classes.npy')\r\n","\r\n","\r\n","filename = in_usoon_priv_database+\"model.pkl\"\r\n","model = pickle.load(open(filename, 'rb'))\r\n","\r\n","\r\n","for i in range(embed_face_test.shape[0]):\r\n","    random_face_emb = embed_face_test[i]\r\n","    samples = expand_dims(random_face_emb, axis=0)\r\n","    yhat_class = model.predict(samples)\r\n","    class_index = yhat_class[0]\r\n","    predict_names = out_encoder.inverse_transform(yhat_class)\r\n","\r\n","    if predict_names[0]!=idd and predict_names[0]!=\"other\":\r\n","      score=face_score_collect[i]\r\n","      print('Predicted: %s' % predict_names[0])\r\n","      print(score)\r\n","      test_data_singular=face_id_collect[i]\r\n","      x1, y1, width, height=face_dimension_collect[i]\r\n","      image_loadedd = cv2.imread(test_data_singular)\r\n","      pyplot.axis('off')\r\n","      pyplot.imshow(image_loadedd)\r\n","      ax = pyplot.gca()\r\n","      rect = Rectangle((x1, y1), width, height, fill=False, color='red')\r\n","      ax.add_patch(rect)\r\n","      # show the plot\r\n","      pyplot.show()\r\n"],"execution_count":null,"outputs":[]}]}